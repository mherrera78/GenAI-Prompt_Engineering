{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af4db419-6706-4039-90f3-391261fd9109",
   "metadata": {},
   "source": [
    "# Assignment # 3:  Invoke models using Bedrock APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c440bcef-2d86-4b63-8711-a7db325cd96e",
   "metadata": {},
   "source": [
    "## Miguel Herrera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ad6f01-a437-4a78-b496-d96566664f01",
   "metadata": {},
   "source": [
    "The following assignment is composed of 2 parts:\n",
    "\n",
    "Section 1) Access models using the **invoke_model()** function\n",
    "\n",
    "Section 2) Access models using the **invoke_model_with_response_stream()** function\n",
    "\n",
    "**Note:** Please expand all sections to see code for each model.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "22b5deb0-b476-447b-a995-6332eb1f3829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#https://python.langchain.com/docs/integrations/chat/bedrock/\n",
    "\n",
    "%pip install -r requirements.txt -Uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "def2dc25-5baa-4766-85fc-c118b9f4a62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "session = boto3.Session()\n",
    "bedrock = session.client(\n",
    "    service_name='bedrock',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4bf0f244-91b1-49e0-ad9c-64d4b09922ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = bedrock.list_foundation_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d7f4cbf0-a201-49cb-aaa5-2addd3e6da36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-tg1-large',\n",
       "  'modelId': 'amazon.titan-tg1-large',\n",
       "  'modelName': 'Titan Text Large',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-g1-text-02',\n",
       "  'modelId': 'amazon.titan-embed-g1-text-02',\n",
       "  'modelName': 'Titan Text Embeddings v2',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['EMBEDDING'],\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-text-lite-v1:0:4k',\n",
       "  'modelId': 'amazon.titan-text-lite-v1:0:4k',\n",
       "  'modelName': 'Titan Text G1 - Lite',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': ['FINE_TUNING', 'CONTINUED_PRE_TRAINING'],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-text-lite-v1',\n",
       "  'modelId': 'amazon.titan-text-lite-v1',\n",
       "  'modelName': 'Titan Text G1 - Lite',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-text-express-v1:0:8k',\n",
       "  'modelId': 'amazon.titan-text-express-v1:0:8k',\n",
       "  'modelName': 'Titan Text G1 - Express',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': ['FINE_TUNING', 'CONTINUED_PRE_TRAINING'],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-text-express-v1',\n",
       "  'modelId': 'amazon.titan-text-express-v1',\n",
       "  'modelName': 'Titan Text G1 - Express',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-text-v1:2:8k',\n",
       "  'modelId': 'amazon.titan-embed-text-v1:2:8k',\n",
       "  'modelName': 'Titan Embeddings G1 - Text',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['EMBEDDING'],\n",
       "  'responseStreamingSupported': False,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-text-v1',\n",
       "  'modelId': 'amazon.titan-embed-text-v1',\n",
       "  'modelName': 'Titan Embeddings G1 - Text',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['EMBEDDING'],\n",
       "  'responseStreamingSupported': False,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-text-v2:0:8k',\n",
       "  'modelId': 'amazon.titan-embed-text-v2:0:8k',\n",
       "  'modelName': 'Titan Text Embeddings V2',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['EMBEDDING'],\n",
       "  'responseStreamingSupported': False,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': [],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-text-v2:0',\n",
       "  'modelId': 'amazon.titan-embed-text-v2:0',\n",
       "  'modelName': 'Titan Text Embeddings V2',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['EMBEDDING'],\n",
       "  'responseStreamingSupported': False,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-image-v1:0',\n",
       "  'modelId': 'amazon.titan-embed-image-v1:0',\n",
       "  'modelName': 'Titan Multimodal Embeddings G1',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['EMBEDDING'],\n",
       "  'customizationsSupported': ['FINE_TUNING'],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-image-v1',\n",
       "  'modelId': 'amazon.titan-embed-image-v1',\n",
       "  'modelName': 'Titan Multimodal Embeddings G1',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['EMBEDDING'],\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-image-generator-v1:0',\n",
       "  'modelId': 'amazon.titan-image-generator-v1:0',\n",
       "  'modelName': 'Titan Image Generator G1',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['IMAGE'],\n",
       "  'customizationsSupported': ['FINE_TUNING'],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-image-generator-v1',\n",
       "  'modelId': 'amazon.titan-image-generator-v1',\n",
       "  'modelName': 'Titan Image Generator G1',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['IMAGE'],\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-image-generator-v2:0',\n",
       "  'modelId': 'amazon.titan-image-generator-v2:0',\n",
       "  'modelName': 'Titan Image Generator G1 v2',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['IMAGE'],\n",
       "  'customizationsSupported': ['FINE_TUNING'],\n",
       "  'inferenceTypesSupported': ['PROVISIONED', 'ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/stability.stable-diffusion-xl-v1:0',\n",
       "  'modelId': 'stability.stable-diffusion-xl-v1:0',\n",
       "  'modelName': 'SDXL 1.0',\n",
       "  'providerName': 'Stability AI',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['IMAGE'],\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/stability.stable-diffusion-xl-v1',\n",
       "  'modelId': 'stability.stable-diffusion-xl-v1',\n",
       "  'modelName': 'SDXL 1.0',\n",
       "  'providerName': 'Stability AI',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['IMAGE'],\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/stability.sd3-large-v1:0',\n",
       "  'modelId': 'stability.sd3-large-v1:0',\n",
       "  'modelName': 'SD3 Large 1.0',\n",
       "  'providerName': 'Stability AI',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['IMAGE'],\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/stability.stable-image-core-v1:0',\n",
       "  'modelId': 'stability.stable-image-core-v1:0',\n",
       "  'modelName': 'Stable Image Core 1.0',\n",
       "  'providerName': 'Stability AI',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['IMAGE'],\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/stability.stable-image-ultra-v1:0',\n",
       "  'modelId': 'stability.stable-image-ultra-v1:0',\n",
       "  'modelName': 'Stable Image Ultra 1.0',\n",
       "  'providerName': 'Stability AI',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['IMAGE'],\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/ai21.j2-grande-instruct',\n",
       "  'modelId': 'ai21.j2-grande-instruct',\n",
       "  'modelName': 'J2 Grande Instruct',\n",
       "  'providerName': 'AI21 Labs',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': False,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/ai21.j2-jumbo-instruct',\n",
       "  'modelId': 'ai21.j2-jumbo-instruct',\n",
       "  'modelName': 'J2 Jumbo Instruct',\n",
       "  'providerName': 'AI21 Labs',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': False,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-instant-v1:2:100k',\n",
       "  'modelId': 'anthropic.claude-instant-v1:2:100k',\n",
       "  'modelName': 'Claude Instant',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-instant-v1',\n",
       "  'modelId': 'anthropic.claude-instant-v1',\n",
       "  'modelName': 'Claude Instant',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2:0:18k',\n",
       "  'modelId': 'anthropic.claude-v2:0:18k',\n",
       "  'modelName': 'Claude',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2:0:100k',\n",
       "  'modelId': 'anthropic.claude-v2:0:100k',\n",
       "  'modelName': 'Claude',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2:1:18k',\n",
       "  'modelId': 'anthropic.claude-v2:1:18k',\n",
       "  'modelName': 'Claude',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2:1:200k',\n",
       "  'modelId': 'anthropic.claude-v2:1:200k',\n",
       "  'modelName': 'Claude',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2:1',\n",
       "  'modelId': 'anthropic.claude-v2:1',\n",
       "  'modelName': 'Claude',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2',\n",
       "  'modelId': 'anthropic.claude-v2',\n",
       "  'modelName': 'Claude',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0:28k',\n",
       "  'modelId': 'anthropic.claude-3-sonnet-20240229-v1:0:28k',\n",
       "  'modelName': 'Claude 3 Sonnet',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0:200k',\n",
       "  'modelId': 'anthropic.claude-3-sonnet-20240229-v1:0:200k',\n",
       "  'modelName': 'Claude 3 Sonnet',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0',\n",
       "  'modelId': 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
       "  'modelName': 'Claude 3 Sonnet',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-haiku-20240307-v1:0:48k',\n",
       "  'modelId': 'anthropic.claude-3-haiku-20240307-v1:0:48k',\n",
       "  'modelName': 'Claude 3 Haiku',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-haiku-20240307-v1:0:200k',\n",
       "  'modelId': 'anthropic.claude-3-haiku-20240307-v1:0:200k',\n",
       "  'modelName': 'Claude 3 Haiku',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-haiku-20240307-v1:0',\n",
       "  'modelId': 'anthropic.claude-3-haiku-20240307-v1:0',\n",
       "  'modelName': 'Claude 3 Haiku',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-opus-20240229-v1:0:12k',\n",
       "  'modelId': 'anthropic.claude-3-opus-20240229-v1:0:12k',\n",
       "  'modelName': 'Claude 3 Opus',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-opus-20240229-v1:0:28k',\n",
       "  'modelId': 'anthropic.claude-3-opus-20240229-v1:0:28k',\n",
       "  'modelName': 'Claude 3 Opus',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-opus-20240229-v1:0:200k',\n",
       "  'modelId': 'anthropic.claude-3-opus-20240229-v1:0:200k',\n",
       "  'modelName': 'Claude 3 Opus',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-opus-20240229-v1:0',\n",
       "  'modelId': 'anthropic.claude-3-opus-20240229-v1:0',\n",
       "  'modelName': 'Claude 3 Opus',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0:18k',\n",
       "  'modelId': 'anthropic.claude-3-5-sonnet-20240620-v1:0:18k',\n",
       "  'modelName': 'Claude 3.5 Sonnet',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0:51k',\n",
       "  'modelId': 'anthropic.claude-3-5-sonnet-20240620-v1:0:51k',\n",
       "  'modelName': 'Claude 3.5 Sonnet',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0:200k',\n",
       "  'modelId': 'anthropic.claude-3-5-sonnet-20240620-v1:0:200k',\n",
       "  'modelName': 'Claude 3.5 Sonnet',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       "  'modelId': 'anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       "  'modelName': 'Claude 3.5 Sonnet',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.command-text-v14:7:4k',\n",
       "  'modelId': 'cohere.command-text-v14:7:4k',\n",
       "  'modelName': 'Command',\n",
       "  'providerName': 'Cohere',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': ['FINE_TUNING'],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.command-text-v14',\n",
       "  'modelId': 'cohere.command-text-v14',\n",
       "  'modelName': 'Command',\n",
       "  'providerName': 'Cohere',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.command-r-v1:0',\n",
       "  'modelId': 'cohere.command-r-v1:0',\n",
       "  'modelName': 'Command R',\n",
       "  'providerName': 'Cohere',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.command-r-plus-v1:0',\n",
       "  'modelId': 'cohere.command-r-plus-v1:0',\n",
       "  'modelName': 'Command R+',\n",
       "  'providerName': 'Cohere',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.command-light-text-v14:7:4k',\n",
       "  'modelId': 'cohere.command-light-text-v14:7:4k',\n",
       "  'modelName': 'Command Light',\n",
       "  'providerName': 'Cohere',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': ['FINE_TUNING'],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.command-light-text-v14',\n",
       "  'modelId': 'cohere.command-light-text-v14',\n",
       "  'modelName': 'Command Light',\n",
       "  'providerName': 'Cohere',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.embed-english-v3:0:512',\n",
       "  'modelId': 'cohere.embed-english-v3:0:512',\n",
       "  'modelName': 'Embed English',\n",
       "  'providerName': 'Cohere',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['EMBEDDING'],\n",
       "  'responseStreamingSupported': False,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.embed-english-v3',\n",
       "  'modelId': 'cohere.embed-english-v3',\n",
       "  'modelName': 'Embed English',\n",
       "  'providerName': 'Cohere',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['EMBEDDING'],\n",
       "  'responseStreamingSupported': False,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.embed-multilingual-v3:0:512',\n",
       "  'modelId': 'cohere.embed-multilingual-v3:0:512',\n",
       "  'modelName': 'Embed Multilingual',\n",
       "  'providerName': 'Cohere',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['EMBEDDING'],\n",
       "  'responseStreamingSupported': False,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/cohere.embed-multilingual-v3',\n",
       "  'modelId': 'cohere.embed-multilingual-v3',\n",
       "  'modelName': 'Embed Multilingual',\n",
       "  'providerName': 'Cohere',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['EMBEDDING'],\n",
       "  'responseStreamingSupported': False,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-13b-chat-v1:0:4k',\n",
       "  'modelId': 'meta.llama2-13b-chat-v1:0:4k',\n",
       "  'modelName': 'Llama 2 Chat 13B',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'LEGACY'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-13b-chat-v1',\n",
       "  'modelId': 'meta.llama2-13b-chat-v1',\n",
       "  'modelName': 'Llama 2 Chat 13B',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'LEGACY'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-70b-chat-v1:0:4k',\n",
       "  'modelId': 'meta.llama2-70b-chat-v1:0:4k',\n",
       "  'modelName': 'Llama 2 Chat 70B',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': [],\n",
       "  'modelLifecycle': {'status': 'LEGACY'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-70b-chat-v1',\n",
       "  'modelId': 'meta.llama2-70b-chat-v1',\n",
       "  'modelName': 'Llama 2 Chat 70B',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'LEGACY'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-13b-v1:0:4k',\n",
       "  'modelId': 'meta.llama2-13b-v1:0:4k',\n",
       "  'modelName': 'Llama 2 13B',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': ['FINE_TUNING'],\n",
       "  'inferenceTypesSupported': [],\n",
       "  'modelLifecycle': {'status': 'LEGACY'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-13b-v1',\n",
       "  'modelId': 'meta.llama2-13b-v1',\n",
       "  'modelName': 'Llama 2 13B',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': [],\n",
       "  'modelLifecycle': {'status': 'LEGACY'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-70b-v1:0:4k',\n",
       "  'modelId': 'meta.llama2-70b-v1:0:4k',\n",
       "  'modelName': 'Llama 2 70B',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': ['FINE_TUNING'],\n",
       "  'inferenceTypesSupported': [],\n",
       "  'modelLifecycle': {'status': 'LEGACY'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama2-70b-v1',\n",
       "  'modelId': 'meta.llama2-70b-v1',\n",
       "  'modelName': 'Llama 2 70B',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': [],\n",
       "  'modelLifecycle': {'status': 'LEGACY'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-8b-instruct-v1:0',\n",
       "  'modelId': 'meta.llama3-8b-instruct-v1:0',\n",
       "  'modelName': 'Llama 3 8B Instruct',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-70b-instruct-v1:0',\n",
       "  'modelId': 'meta.llama3-70b-instruct-v1:0',\n",
       "  'modelName': 'Llama 3 70B Instruct',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-1-8b-instruct-v1:0',\n",
       "  'modelId': 'meta.llama3-1-8b-instruct-v1:0',\n",
       "  'modelName': 'Llama 3.1 8B Instruct',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-1-70b-instruct-v1:0',\n",
       "  'modelId': 'meta.llama3-1-70b-instruct-v1:0',\n",
       "  'modelName': 'Llama 3.1 70B Instruct',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-1-405b-instruct-v1:0',\n",
       "  'modelId': 'meta.llama3-1-405b-instruct-v1:0',\n",
       "  'modelName': 'Llama 3.1 405B Instruct',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-2-11b-instruct-v1:0',\n",
       "  'modelId': 'meta.llama3-2-11b-instruct-v1:0',\n",
       "  'modelName': 'Llama 3.2 11B Instruct',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-2-90b-instruct-v1:0',\n",
       "  'modelId': 'meta.llama3-2-90b-instruct-v1:0',\n",
       "  'modelName': 'Llama 3.2 90B Instruct',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-2-1b-instruct-v1:0',\n",
       "  'modelId': 'meta.llama3-2-1b-instruct-v1:0',\n",
       "  'modelName': 'Llama 3.2 1B Instruct',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/meta.llama3-2-3b-instruct-v1:0',\n",
       "  'modelId': 'meta.llama3-2-3b-instruct-v1:0',\n",
       "  'modelName': 'Llama 3.2 3B Instruct',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['INFERENCE_PROFILE'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/mistral.mistral-7b-instruct-v0:2',\n",
       "  'modelId': 'mistral.mistral-7b-instruct-v0:2',\n",
       "  'modelName': 'Mistral 7B Instruct',\n",
       "  'providerName': 'Mistral AI',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/mistral.mixtral-8x7b-instruct-v0:1',\n",
       "  'modelId': 'mistral.mixtral-8x7b-instruct-v0:1',\n",
       "  'modelName': 'Mixtral 8x7B Instruct',\n",
       "  'providerName': 'Mistral AI',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/mistral.mistral-large-2402-v1:0',\n",
       "  'modelId': 'mistral.mistral-large-2402-v1:0',\n",
       "  'modelName': 'Mistral Large (2402)',\n",
       "  'providerName': 'Mistral AI',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/mistral.mistral-large-2407-v1:0',\n",
       "  'modelId': 'mistral.mistral-large-2407-v1:0',\n",
       "  'modelName': 'Mistral Large (2407)',\n",
       "  'providerName': 'Mistral AI',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text['modelSummaries']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8804d3a6-db52-4bb7-820e-d5c792674d17",
   "metadata": {},
   "source": [
    "## ---------- Creating the Bedrock runtime client to be used by all functions below ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bdf42752-2fbd-4d02-b840-39ab4eb38daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup bedrock runtime client for invoking models\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4983c2a5-6d4f-4060-8403-583a2c321acf",
   "metadata": {},
   "source": [
    "# SECTION 1: Invoking Models (Invoke Model) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4987786-a41e-47f3-838f-2daa585efa01",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Antropic Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "8ae01d19-83b7-470b-8222-88659a1b2fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_invoke(prompt):\n",
    "\n",
    "\n",
    "    modelId = \"anthropic.claude-v2:1\"\n",
    "    \n",
    "    #configuring the payload\n",
    "    prompt_config = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 4096,\n",
    "        \"temperature\":0.1,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    body = json.dumps(prompt_config)\n",
    "\n",
    "    #modelId = [\"anthropic.claude-3-sonnet-20240229-v1:0\",\"anthropic.claude-3-haiku-20240307-v1:0\", \"anthropic.claude-v2:1\"]\t\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    results = response_body.get(\"content\")[0].get(\"text\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "e131ab19-0792-4ac9-9630-045247274c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI. Using Amazon Bedrock, you can easily experiment with and evaluate top FMs for your use case, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources. Since Amazon Bedrock is serverless, you don't have to manage any infrastructure, and you can securely integrate and deploy generative AI capabilities into your applications using the AWS services you are already familiar with\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "f7cf8419-dafb-4697-b2f4-29090724511f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The text states that breakfast is the most important meal of the day, possibly in a sarcastic tone.\n",
      "\n",
      "Sarcasm score: 6/10\n",
      "\n",
      "The text is short and the claim about breakfast being the most important meal is commonly said sincerely. However, the exclamation point suggests it could be meant sarcastically. Without more context, it's hard to tell definitively.\n",
      "\n",
      "\n",
      "Here is a 47-word summary of the text:\n",
      "\n",
      "The text states that the sky on a sunny day is blue. There is not enough context to determine if this comment is intended to be sarcastic. I would rate the level of sarcasm as 1 out of 5.\n"
     ]
    }
   ],
   "source": [
    "# Summarization\n",
    "\n",
    "prompt = \"Can you give me 2 real life applications of RAG systems.\"\n",
    "\n",
    "text = \"Breakfast in bed is the right way to start the day!\"\n",
    "text = \"Breakfast in the most important meal of the day!\"\n",
    "\n",
    "prompt = f\"Summarize the following text in 50 words or less.  Keep in mind that the comment might be sarcastic in nature or not.  Also provide a score of how sarcastic the comment might be: {text}\"\n",
    "\n",
    "print(model_invoke(prompt))\n",
    "print()\n",
    "print()\n",
    "\n",
    "text = \"The sky on a sunny day is blue!\"\n",
    "prompt = f\"Summarize the following text in 50 words or less.  Keep in mind that the comment might be sarcastic in nature or not.  Also provide a score of how sarcastic the comment might be: {text}\"\n",
    "\n",
    "print(model_invoke(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "75eeefb1-c93e-4d2c-a6fb-d085d749e528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"sentiment\": \"positive\"}\n"
     ]
    }
   ],
   "source": [
    "#Sentiment\n",
    "\n",
    "text=\"Peaches from Georgia are the okay!\"\n",
    "\n",
    "prompt = f\"Giving the following text, return only a valid JSON object of sentiment analysis. skip the preamble text: {text} \"\n",
    "print(model_invoke(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "136e1d69-10f4-4456-8d2b-b30ad87cec96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: What is Amazon Bedrock?\n",
      "Response : Unfortunately, the text does not mention anything about \"Amazon Bedrock\". I do not know what Amazon Bedrock is based on the given text. The text talks about peaches from Georgia, but does not provide any information to answer the question about Amazon Bedrock. \n",
      "\n",
      "question: How many models does Amazon Bedrock offer?\n",
      "Response : Unfortunately, the given text does not provide enough information to determine how many models Amazon Bedrock offers. The text talks about peaches from Georgia, but does not mention Amazon Bedrock or any models it might offer. So I do not know how many models Amazon Bedrock offers based on this text. \n",
      "\n",
      "question:  Does Amazon Bedrock support RAG?\n",
      "Response : Unfortunately, the given text does not provide enough information to determine whether Amazon Bedrock supports RAG. The text talks about peaches from Georgia, but does not mention Amazon Bedrock or RAG. So I do not know if Amazon Bedrock supports RAG based on this text. \n",
      "\n",
      "question: who is brad pitt?\n",
      "Response : Unfortunately, the given text does not mention who Brad Pitt is. The text talks about peaches from Georgia, but does not provide any information about Brad Pitt. Since the answer is not in the text, I do not know who Brad Pitt is based on this information. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question and Answering\n",
    "question = [\"What is Amazon Bedrock?\",\"How many models does Amazon Bedrock offer?\", \" Does Amazon Bedrock support RAG?\", \"who is brad pitt?\"]\t\n",
    "\n",
    "for i in question:\n",
    "    prompt = f\"Given the following text, answer the question. If the answer is not in the text, 'say you do not know': {i} text: {text} \"\n",
    "    print(f\"question: {i}\")\n",
    "    print(f\"Response : {model_invoke(prompt)} \\n\")\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3be6dff-7fcd-452f-af0a-68d219c15e90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## AI21 Labs Jurassic ---  ** No models available on Bedrock for AI21 Labs **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4921cc2a-8cac-435e-85e0-8eacfbf2a3bc",
   "metadata": {},
   "source": [
    "**The code below returns error message because Bedrock does not have an AI21 Labs models available.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "0fef95c2-e854-4842-bc9f-c35189ab7335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_jurassic_model(bedrock_runtime, prompt):\n",
    "    # Define the request body\n",
    "    \n",
    "    #model_id= \"ai21.j2-grande-instruct\" #this one does not work\n",
    "    model_id= \"ai21.j2-mid-v1\"\n",
    "    \n",
    "    max_tokens=100\n",
    "    temperature=0.3\n",
    "    \n",
    "    \n",
    "    body = {\n",
    "        \"prompt\": prompt,\n",
    "        \"maxTokens\": max_tokens,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "\n",
    "    # Define the accept and content type headers\n",
    "    accept = \"application/json\"\n",
    "    content_type = \"application/json\"\n",
    "\n",
    "    # Invoke the model using bedrock_runtime\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=json.dumps(body), \n",
    "        modelId=model_id, \n",
    "        accept=accept, \n",
    "        contentType=content_type\n",
    "    )\n",
    "\n",
    "    # Parse the response body\n",
    "    response_body = json.loads(response[\"body\"].read().decode('utf-8'))\n",
    "\n",
    "    return response_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "90f078c3-5586-4451-bf31-d8d2932e0809",
   "metadata": {},
   "outputs": [
    {
     "ename": "AccessDeniedException",
     "evalue": "An error occurred (AccessDeniedException) when calling the InvokeModel operation: You don't have access to the model with the specified model ID.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAccessDeniedException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[351], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite a short story about a robot learning to paint.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43minvoke_jurassic_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbedrock_runtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[0;32mIn[350], line 22\u001b[0m, in \u001b[0;36minvoke_jurassic_model\u001b[0;34m(bedrock_runtime, prompt)\u001b[0m\n\u001b[1;32m     19\u001b[0m content_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Invoke the model using bedrock_runtime\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mbedrock_runtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodelId\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontentType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_type\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Parse the response body\u001b[39;00m\n\u001b[1;32m     30\u001b[0m response_body \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:565\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:1021\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1019\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mAccessDeniedException\u001b[0m: An error occurred (AccessDeniedException) when calling the InvokeModel operation: You don't have access to the model with the specified model ID."
     ]
    }
   ],
   "source": [
    "prompt = \"Write a short story about a robot learning to paint.\"\n",
    "\n",
    "response = invoke_jurassic_model(bedrock_runtime, prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb173f2-6774-460e-80d4-669bc4e71c54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Amazon Titan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "39d5d787-1a10-4e5d-aa32-039c234feed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def invoke_titan_model2(bedrock_runtime, prompt, max_tokens=100, temperature=0.1):\n",
    "   \n",
    "    # Define the request body\n",
    "    model_id=\"amazon.titan-text-lite-v1\"\n",
    "        \n",
    "    body = {\n",
    "        \"inputText\": prompt\n",
    "    }\n",
    "    \n",
    "    # Define the accept and content type headers\n",
    "    accept = \"application/json\"\n",
    "    content_type = \"application/json\"\n",
    "    \n",
    "    # Invoke the model using bedrock_runtime\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=json.dumps(body), \n",
    "        modelId=model_id,  \n",
    "        accept=accept, \n",
    "        contentType=content_type\n",
    "    )\n",
    "    \n",
    "    # Parse the response body\n",
    "    response_body = json.loads(response[\"body\"].read().decode('utf-8'))\n",
    "    \n",
    "    return response_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "6dc5e4a1-048d-41c8-8947-151cc0e64c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here are some steps to get a visa to Australia:\n",
      "\n",
      "1. Determine the type of visa you need.\n",
      "2. Gather the required documents.\n",
      "3. Complete the visa application form.\n",
      "4. Pay the visa application fee.\n",
      "5. Schedule an interview if required.\n",
      "6. Attend your visa interview.\n",
      "7. Wait for a decision on your visa application.\n",
      "\n",
      "\n",
      "question: What is Amazon Bedrock?\n",
      "Response : \n",
      "Amazon Bedrock is a managed service that makes foundation models from leading AI startup and Amazon's own Titan models available through APIs. The Amazon Bedrock service makes it easy to get started with AI. You can get started for free with Amazon Bedrock and connect your existing tools and favorite Amazon Web Services (AWS) tools.  \n",
      "\n",
      "question: How many models does Amazon Bedrock offer?\n",
      "Response : \n",
      "Amazon Bedrock is a managed service that makes foundation models from leading AI startup and Amazon's own Titan models available through APIs. You can visit the bedrock product page to learn more. \n",
      "\n",
      "question:  Does Amazon Bedrock support RAG?\n",
      "Response : \n",
      "Amazon SageMaker Autopilot can automatically train your models and tune hyperparameters for improved performance. For more information on Amazon SageMaker Autopilot, see the documentation. \n",
      "\n",
      "question: who is brad pitt?\n",
      "Response : \n",
      "Brad Pitt is an American actor and filmmaker. He has appeared in more than 25 films that have grossed over $1.3 billion worldwide. He is known for his portrayal of antihero characters, such as Tyler Durden in the \"Fight Club\" trilogy, a role for which he won the Golden Globe Award for Best Actor. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"what is the simplest way to get a visa to australia.\"\n",
    "response = invoke_titan_model2(bedrock_runtime, prompt)\n",
    "print(response['results'][0]['outputText'])\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "question = [\"What is Amazon Bedrock?\",\"How many models does Amazon Bedrock offer?\", \" Does Amazon Bedrock support RAG?\", \"who is brad pitt?\"]\t\n",
    "\n",
    "for i in question:\n",
    "    prompt = f\"Given the following text, answer the question. If the answer is not in the text, 'say you do not know': {i} text: {text} \"\n",
    "    print(f\"question: {i}\")\n",
    "    print(f\"Response : {invoke_titan_model2(bedrock_runtime, i)['results'][0]['outputText']} \\n\")\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "42061a11-5a16-4671-99ce-3aed0db0c828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the sentiment analysis of the provided text:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"sentiment\": \"positive\",\n",
      "    \"comparative\": \"best\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "text = \"Peaches from Georgia are the best in the country!\"\n",
    "\n",
    "prompt = f\"Giving the following text, return the result of the sentiment analysis formetted as a json file. skip the preamble text: {text} \"\n",
    "print(invoke_titan_model2(bedrock_runtime, prompt)['results'][0]['outputText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a00f25-8f58-4948-897e-39bfbb89cbda",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Cohere - Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "0473d80b-c0c7-4189-9e84-35ee49cc11be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def invoke_cohere_model(bedrock_runtime, prompt):\n",
    "    \n",
    "    model_id=\"cohere.command-light-text-v14\"  #This model works with the payload configuration below.\n",
    "\n",
    "    ## Tried with some of the models below, most of them gave me an error message\n",
    "    \n",
    "    #cohere.command-r-plus-v1:0\n",
    "    #cohere.command-text-v14\n",
    "    #cohere.command-light-text-v14\n",
    "    #cohere.command-r-plus-v1:0\n",
    "    #cohere.command-r-v1:0\n",
    "    \n",
    "    # Setting temperature at 0.9 to allow the model to be more \"creative\"\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.9\n",
    "    }\n",
    "    \n",
    "    # Define the accept and content type headers\n",
    "    accept = \"application/json\"\n",
    "    content_type = \"application/json\"\n",
    "    \n",
    "    # Invoke the model using bedrock_runtime\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=json.dumps(payload), \n",
    "        modelId=model_id,\n",
    "        accept=accept, \n",
    "        contentType=content_type\n",
    "    )\n",
    "    \n",
    "    # Parse the response body\n",
    "    response_body = json.loads(response[\"body\"].read().decode('utf-8'))\n",
    "    \n",
    "    return response_body\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "9bef563e-09bb-4a24-9284-f16f025c3d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Wolf in sheep's clothing:\n",
      "\n",
      "The girl in green\n",
      "Came across the valley\n",
      "Had to traverse it in the dark\n",
      "She did not want to get murdered like last time\n",
      "So she opted for the risky journey on foot\n",
      "Under the full moon, she saw lights, quite bright\n",
      "She slowed down, and then she heard:\n",
      "\n",
      "Ding dong!\n",
      "Who is there?\n",
      "A kindly voice called out\n",
      "“Please open the door, I am no danger, don’t fear!”\n",
      "\n",
      "So, some smarts and courage, the girl opened the door, and what a surprise!\n",
      "It was the wolf in the sheep’s skin, or so it seemed.\n",
      "\n",
      "A warm welcome, and tea prepared,\n",
      "The story she told, of a “wicked” family member.\n",
      "The old fox ready with a plan,\n",
      "To prove her wrong and save the day, what joy!\n",
      "\n",
      "So, the moral of the story is this:\n",
      "Look carefully, and you may find:\n",
      "The good in the bad, and the bad in the good\n",
      "Be they clothes, or people, it’s hard to tell!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Rewrite the Red Riding Hood story but having the wolf being the good guy. The story should only be a summary.\"\n",
    "\n",
    "response = invoke_cohere_model(bedrock_runtime, prompt)\n",
    "\n",
    "print(response['generations'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc182ec-a4e7-47b8-98e7-16b6dc92341d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "38550596-4849-4eb9-b1c2-0b99436b331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def invoke_llama3_model(bedrock_runtime, prompt):\n",
    "    \n",
    "    model_id=\"meta.llama3-70b-instruct-v1:0\"  \n",
    "    \n",
    "    ## Both of these models work\n",
    "    \n",
    "    #meta.llama3-70b-instruct-v1:0\n",
    "    #meta.llama3-8b-instruct-v1:0\n",
    "    \n",
    "    formatted_prompt = f\"\"\"\n",
    "    <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "    {prompt}\n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setting temperature at 0.9 to allow the model to be more \"creative\"\n",
    "    payload = {\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"max_gen_len\": 512,\n",
    "        \"temperature\": 0.5,\n",
    "    }\n",
    "    \n",
    "    # Define the accept and content type headers\n",
    "    accept = \"application/json\"\n",
    "    content_type = \"application/json\"\n",
    "    \n",
    "    # Invoke the model using bedrock_runtime\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=json.dumps(payload), \n",
    "        modelId=model_id,\n",
    "        accept=accept, \n",
    "        contentType=content_type\n",
    "    )\n",
    "    \n",
    "    # Parse the response body\n",
    "    response_body = json.loads(response[\"body\"].read().decode('utf-8'))\n",
    "    \n",
    "    return response_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "52e14c3e-8574-479e-9efb-6c3b5b35ae40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is a rewritten summary of Red Riding Hood where the wolf is the good guy:\n",
      "\n",
      "In this version of the classic tale, the wolf is not a villain, but a hero who saves the day. Little Red Riding Hood, a kind and gentle soul, is on her way to visit her sick grandmother in the woods. However, the woods are fraught with danger, and a wicked woodsman has been terrorizing the forest, stealing food from the other animals and threatening their safety.\n",
      "\n",
      "The wolf, who has been watching over the forest and its inhabitants, discovers the woodsman's evil plans and decides to intervene. He cleverly distracts the woodsman, allowing Little Red Riding Hood to safely reach her grandmother's house.\n",
      "\n",
      "When the woodsman tries to attack Grandmother's house, the wolf bravely chases him away, saving the day and earning the gratitude of Little Red Riding Hood and her grandmother. The wolf's bravery and kindness are rewarded, and he becomes a beloved member of the forest community, celebrated for his heroism and selflessness.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Rewrite the Red Riding Hood story but having the wolf being the good guy. The story should only be a summary.\"\n",
    "\n",
    "response = invoke_llama3_model(bedrock_runtime, prompt)\n",
    "\n",
    "print(response['generation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "2b43eea3-64ce-491d-860b-3d7910951cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are two real-life applications of RAG (Resource Allocation and Gantt Chart) systems:\n",
      "\n",
      "**1. Construction Project Management**\n",
      "\n",
      "A construction company is building a new high-rise office building in a city center. The project involves multiple tasks, such as excavation, foundation work, structural construction, electrical and plumbing installation, and finishing work (e.g., drywall, flooring, and painting). The project manager uses a RAG system to plan and schedule the tasks, allocate resources (e.g., labor, equipment, and materials), and track progress.\n",
      "\n",
      "The RAG system helps the project manager to:\n",
      "\n",
      "* Identify dependencies between tasks and create a logical sequence of activities\n",
      "* Allocate resources to each task, ensuring that the right personnel and equipment are assigned to each activity\n",
      "* Develop a Gantt chart to visualize the project schedule and identify potential bottlenecks and delays\n",
      "* Monitor progress and adjust the schedule as needed to ensure the project is completed on time and within budget\n",
      "\n",
      "**2. Manufacturing Production Planning**\n",
      "\n",
      "A manufacturing company produces customized machinery for various industries. The production process involves multiple stages, including design, prototyping, testing, and assembly. The production planner uses a RAG system to plan and schedule the production process, allocate resources (e.g., machinery, labor, and materials), and track progress.\n",
      "\n",
      "The RAG system helps the production planner to:\n",
      "\n",
      "* Identify dependencies between production stages and create a logical sequence of activities\n",
      "* Allocate resources to each stage, ensuring that the right machinery and labor are assigned to each activity\n",
      "* Develop a Gantt chart to visualize the production schedule and identify potential bottlenecks and delays\n",
      "* Monitor progress and adjust the schedule as needed to ensure that customer orders are fulfilled on time and to the required quality standards.\n",
      "\n",
      "In both examples, the RAG system helps to optimize resource allocation, reduce delays and bottlenecks, and improve overall efficiency and productivity.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Can you give me 2 real life applications of RAG systems.\"\n",
    "\n",
    "response = invoke_llama3_model(bedrock_runtime, prompt)\n",
    "\n",
    "print(response['generation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5efee64-bf06-492c-8658-5efe352afab5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "103e1dcf-0768-4cc2-b79d-26938f6cf602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def invoke_mistral_model(bedrock_runtime, prompt):\n",
    "    \n",
    "    model_id=\"mistral.mistral-large-2402-v1:0\"\n",
    "    \n",
    "    #amazon.titan-text-lite-v1\n",
    "    #cohere.command-r-v1:0\n",
    "    \n",
    "    formatted_prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
    "    \n",
    "    # Define the request payload\n",
    "    payload = {\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.1,\n",
    "    }\n",
    "    \n",
    "    # Define the accept and content type headers\n",
    "    accept = \"application/json\"\n",
    "    content_type = \"application/json\"\n",
    "    \n",
    "    # Invoke the model using bedrock_runtime\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=json.dumps(payload), \n",
    "        modelId=model_id,\n",
    "        accept=accept, \n",
    "        contentType=content_type\n",
    "    )\n",
    "    \n",
    "    # Parse the response body\n",
    "    response_body = json.loads(response[\"body\"].read().decode('utf-8'))\n",
    "    \n",
    "    return response_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "d7bb1e3c-aa11-4b34-b7d9-40ed91bb6960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, here is a Python function that should do what you're asking for:\n",
      "\n",
      "```python\n",
      "def create_matrix(array, integer):\n",
      "    # Get the length of the array\n",
      "    n = len(array)\n",
      "    # Create an empty matrix with n rows and m columns\n",
      "    matrix = [[0] * integer for _ in range(n)]\n",
      "    # Fill the matrix with the multiplication of each array member by the integer\n",
      "    for i in range(n):\n",
      "        for j in range(integer):\n",
      "            matrix[i][j] = array[i] * integer\n",
      "    # Return the matrix\n",
      "    return matrix\n",
      "```\n",
      "\n",
      "You can use this function like this:\n",
      "\n",
      "```python\n",
      "numbers = [1, 2, 3, 4, 5]\n",
      "integer = 3\n",
      "print(create_matrix(numbers, integer))\n",
      "```\n",
      "\n",
      "This will output:\n",
      "\n",
      "```python\n",
      "[[3, 3, 3], [6, 6, 6], [9, 9, 9], [12, 12, 12], [15, 15, 15]]\n",
      "```\n",
      "\n",
      "Each row of the matrix is the corresponding number from the array multiplied by the integer.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Rewrite the Red Riding Hood story but having the wolf being the good guy. The story should only be a summary.\"\n",
    "\n",
    "prompt = \"Write a function in python that gets an array of numbers and an integer number as parameters and returns a matrix of n rows and m columns where n is the lenght of the elements in the array and m is the integer number.  The cells should be calculated as the multiplication from each array member by the integer given as parameter\"\n",
    "\n",
    "\n",
    "response = invoke_mistral_model(bedrock_runtime, prompt)\n",
    "\n",
    "print(response[\"outputs\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3268438f-fdbd-4641-a1ea-68c79f7a89bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Stability AI - Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "d1b30e29-53db-4482-9375-320ab9a6bfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import base64\n",
    "\n",
    "def invoke_Stable_difusion_model(bedrock_runtime, prompt):\n",
    "    \n",
    "    model_id=\"stability.stable-diffusion-xl-v1\"\n",
    "    \n",
    "    # Generate a random seed.\n",
    "    seed = random.randint(0, 4294967295)\n",
    "    \n",
    "    # Define the request payload\n",
    "    payload = {\n",
    "        \"text_prompts\": [{\"text\": prompt}],\n",
    "        \"style_preset\": \"photographic\",\n",
    "        \"seed\": seed,\n",
    "        \"cfg_scale\": 10,\n",
    "        \"steps\": 30,\n",
    "    }\n",
    "    \n",
    "    # Define the accept and content type headers\n",
    "    accept = \"application/json\"\n",
    "    content_type = \"application/json\"\n",
    "    \n",
    "    # Invoke the model using bedrock_runtime\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=json.dumps(payload), \n",
    "        modelId=model_id,\n",
    "        accept=accept, \n",
    "        contentType=content_type\n",
    "    )\n",
    "    \n",
    "    # Parse the response body\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "    \n",
    "    return response_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "dac54ca8-a120-4b6f-85aa-a7f43d291f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated image has been saved to output/result_2.png\n"
     ]
    }
   ],
   "source": [
    "#prompt = \"A pink monkey riding a camel in the city\"\n",
    "prompt = \"A cartoon soda can walking on an busy street, holding a sword in his hand\"\n",
    "\n",
    "\n",
    "response = invoke_Stable_difusion_model(bedrock_runtime, prompt)\n",
    "\n",
    "base64_image_data = response[\"artifacts\"][0][\"base64\"]\n",
    "\n",
    "#print(response[\"artifacts\"][0][\"base64\"])\n",
    "\n",
    "# Save the generated image to a local folder.\n",
    "i, output_dir = 1, \"output\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "while os.path.exists(os.path.join(output_dir, f\"stability_{i}.png\")):\n",
    "    i += 1\n",
    "\n",
    "image_data = base64.b64decode(base64_image_data)\n",
    "\n",
    "image_path = os.path.join(output_dir, f\"result_{i}.png\")\n",
    "with open(image_path, \"wb\") as file:\n",
    "    file.write(image_data)\n",
    "\n",
    "print(f\"The generated image has been saved to {image_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94986987-d101-430a-82e1-c71cfc197d9a",
   "metadata": {},
   "source": [
    "## --------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f437535-b205-4a3e-bbfb-ddbf096ab9be",
   "metadata": {},
   "source": [
    "# SECTION 2:  Invoking models - (Invoke model with response stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a3634-eee6-4939-b984-11ccd665b0e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Amazon Titan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "557033db-41de-4c7b-8d3b-8a5debfca4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning a new language requires time, dedication, and practice. One of the most efficient ways to learn a language is to immerse yourself in the culture and language. This can involve traveling to a country where the language is spoken, watching movies and TV shows in that language, or even finding language exchange partners. \n",
      "\n",
      "Another effective method is to use language learning apps, such as Duolingo or Babbel. These apps provide interactive lessons and exercises that help you learn vocabulary, grammar, and sentence structure. You can also use language learning podcasts, such as Coffee Break Spanish or Pimsleur. \n",
      "\n",
      "Reading books, newspapers, and magazines in the target language is also a great way to improve your language skills. You can start by reading simple books and gradually work your way up to more complex texts. \n",
      "\n",
      "In addition to these methods, practicing with a language partner can be extremely beneficial. You can practice speaking, listening, and writing with a native speaker or someone who is fluent in the language you are learning. This can help you improve your pronunciation and grammar. \n",
      "\n",
      "It's important to remember that learning a new language takes time and effort. Be patient with yourself and celebrate your progress along the way. You can also find language learning communities online or in your local area to connect with other language learners and exchange ideas. \n",
      "\n",
      "In conclusion, immerse yourself in the culture and language, use language learning apps, read books and newspapers, practice with a language partner, and be patient with yourself. These methods can help you learn a new language efficiently and effectively."
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output, display, display_markdown, Markdown\n",
    "\n",
    "import time\n",
    "\n",
    "prompt = \"Describe what is the most efficient way to learn a new language.\"\n",
    "\n",
    "modelId = \"amazon.titan-tg1-large\"\n",
    "\n",
    "#Both of these 2 models work.\n",
    "\n",
    "#amazon.titan-text-lite-v1\n",
    "#amazon.titan-tg1-large\n",
    "\n",
    "#configuring the payload\n",
    "\n",
    "payload = {\n",
    "    \"inputText\": prompt,\n",
    "    \"textGenerationConfig\": {\n",
    "        \"maxTokenCount\": 512,\n",
    "        \"temperature\": 0.9,\n",
    "    },\n",
    "}\n",
    "\n",
    "body = json.dumps(payload)\n",
    "\n",
    "#accept = \"application/json\"\n",
    "#contentType = \"application/json\"\n",
    "\n",
    "response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "    body=body, modelId=modelId\n",
    ")\n",
    "\n",
    "\n",
    "# Extract and print the response text in real-time.\n",
    "for event in response[\"body\"]:\n",
    "    chunk = json.loads(event[\"chunk\"][\"bytes\"])\n",
    "    if \"outputText\" in chunk:\n",
    "        print(chunk[\"outputText\"], end=\"\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52de66d-5fa7-425f-9879-1cda38a9acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "###  This code also presents the output on a stream.\n",
    "\n",
    "#stream = response.get('body')\n",
    "#output = []\n",
    "\n",
    "#if stream:\n",
    "#    for event in stream:\n",
    "#        chunk = event.get('chunk')\n",
    "#        if chunk:\n",
    "#            chunk_obj = json.loads(chunk.get('bytes').decode())\n",
    "#            text = chunk_obj['outputText']\n",
    "#            clear_output(wait=True)\n",
    "#            output.append(text)\n",
    "#            display_markdown(Markdown(''.join(output)))\n",
    "         \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3a489b-ace4-4c78-a391-1ba9cc428f8f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Antropic - claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "fd446412-107a-4f9f-8cdd-f04ad8989271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Neural networks are trained using a process called backpropagation, which involves adjusting the weights of the connections between the neurons in the network to minimize the error between the network's output and the desired output.\n",
      "\n",
      "Here's a summary of how neural networks are trained:\n",
      "\n",
      "1. **Initialization**: The weights of the connections between the neurons in the network are initialized to small random values. This is done to break the symmetry and ensure that the network can learn different features.\n",
      "\n",
      "2. **Forward Propagation**: The input data is fed into the network, and the output is calculated by propagating the input through the network layer by layer. This is called the forward propagation step.\n",
      "\n",
      "3. **Error Calculation**: The output of the network is compared to the desired output (the ground truth), and the error is calculated using a loss function, such as mean squared error or cross-entropy.\n",
      "\n",
      "4. **Backpropagation**: The error is then propagated backwards through the network, starting from the output layer and moving towards the input layer. This is the backpropagation step, and it is used to compute the gradients of the loss function with respect to the weights of the network.\n",
      "\n",
      "5. **Weight Update**: The gradients computed during the backpropagation step are used to update the weights of the connections between the neurons in the network. This is typically done using an optimization algorithm, such as gradient descent or its variants (e.g., stochastic gradient descent, Adam, RMSProp).\n",
      "\n",
      "The backpropagation process works as follows:\n",
      "\n",
      "1. **Partial Derivatives**: The backpropagation algorithm computes the partial derivatives of the loss function with respect to the weights of the network. These partial derivatives represent the rate of change of the loss function with respect to each weight.\n",
      "\n",
      "2. **Chain Rule**: The backpropagation algorithm uses the chain rule of calculus to compute these partial derivatives. The chain rule allows the algorithm to propagate the error backwards through the network, layer by layer.\n",
      "\n",
      "3. **Weight Update**: The partial derivatives computed during the backpropagation step are used to update the weights of the connections between the neurons in the network. The weights are updated in the direction that reduces the error, using an optimization algorithm such as gradient descent.\n",
      "\n",
      "This process of forward propagation, error calculation, backpropagation, and weight update"
     ]
    }
   ],
   "source": [
    "\n",
    "  \n",
    "#bedrock = boto3.client(service_name='bedrock-runtime')\n",
    "prompt = \"Can you give me 2 real life applications of RAG systems.\"\n",
    "prompt = \"Can you give me summary of how neural networks are trainned. How the weights are calculated and the backpropagation process.\"\n",
    "\n",
    "    \n",
    "model_Id=\"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "    #anthropic.claude-3-haiku-20240307-v1:0\n",
    "    #anthropic.claude-3-sonnet-20240229-v1:0\n",
    "\n",
    "\n",
    "request_body = {\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 512,\n",
    "    \"temperature\":0.2,\n",
    "    \"messages\": [\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [{\"type\": \"text\", \"text\": prompt}],\n",
    "      }\n",
    "    ], \n",
    "    }\n",
    "body = json.dumps(request_body)\n",
    "\n",
    "    \n",
    "# Invoke the Anthropic Claude Sonnet model\n",
    "response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "    modelId=model_Id,\n",
    "    body=body\n",
    ")\n",
    "\n",
    "# Extract and print the response text in real-time.\n",
    "for event in response[\"body\"]:\n",
    "    chunk = json.loads(event[\"chunk\"][\"bytes\"])\n",
    "    if chunk[\"type\"] == \"content_block_delta\":\n",
    "        print(chunk[\"delta\"].get(\"text\", \"\"), end=\"\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eff82b-7025-4697-a7da-66b510c20f7d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Cohere - Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "61a1457b-e62c-4341-86e2-20c4e1e4a031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Second World War started in September 1939, when Germany, under Chancellor Adolf Hitler, invaded Poland from the west and from the east together with their allied partners, the Soviet Union, led by Joseph Stalin. This was the final step of their plan to occupy the country and eliminate its capital, Warsaw, from the map. This attack started the whole war, as Great Britain, which had been defied by Hitler since the rise to power in Germany, declared war on Germany after the latter’s failure to respond to the demand to abstain from further aggression. The Soviet Union, seeing the danger, invaded from the east, and the German army could not stop them.\n",
      "\n",
      "Later in the year, Germany also attacked Denmark and Norway, which were easily conquered due to their neutrality and good relationships with the Axis powers, especially Germany. After that, Germany began its campaign in the East, against Russia, and by May 1940, the Germans had advanced deep into the Soviet Union, reaching the River boundary of the Soviet Union, the border with the Soviet Republic of Estonia, and had also conquered France, after a failed French counter-offensive at the Battle of France.\n",
      "\n",
      "After this victory, Germany decided to invade the United Kingdom, which had been prepared for war but was not expecting an attack from the air, as the Germans had broken the British military secrets and knew the British had no significant air force. The Battle of Britain, which lasted from July to October 1940, saw the German airforce, the Luftwaffe, facing off against the British Royal Air Force. This battle marked the beginning of the German failure to break the British military power. Finally, on June 6, 1944, with the D-Day invasion in France, the Allied forces finally overran the German army in Europe, leading to the end of the war in Europe and the defeat of Germany."
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Set the model ID, e.g., Command R.\n",
    "model_id = \"cohere.command-light-text-v14\"\n",
    "\n",
    "# Define the prompt for the model.\n",
    "prompt = \"write a summary of how the second world war started.\"\n",
    "\n",
    "# Format the request payload using the model's native structure.\n",
    "payload = {\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens\": 2000,\n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "# Convert the native request to JSON.\n",
    "request = json.dumps(payload)\n",
    "\n",
    "# Invoke the model with the request.\n",
    "streaming_response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "    modelId=model_id, body=request\n",
    ")\n",
    "\n",
    "# Extract and print the response text in real-time.\n",
    "for event in streaming_response[\"body\"]:\n",
    "    chunk = json.loads(event[\"chunk\"][\"bytes\"])\n",
    "    if \"generations\" in chunk:\n",
    "        print(chunk[\"generations\"][0][\"text\"], end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ad249e-ad88-40bb-8222-d07a8d6bab06",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "cb04b86a-566f-4d77-addf-dc9f30dad073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine you're running a restaurant, and you have a team of employees who take orders, prepare food, and serve customers. Each employee has a specific role, and they work together to get the job done.\n",
      "\n",
      "In a neural network, the \"employees\" are called \"neurons\" or \"nodes.\" They receive information, process it, and then send it to the next employee (node) in the chain. The \"orders\" they receive are called \"inputs,\" and the \"food\" they produce is called the \"output.\"\n",
      "\n",
      "The key to a neural network is the \"activation function,\" which is like the employee's \"recipe book.\" It tells each employee (node) how to process the information they receive and what to do with it.\n",
      "\n",
      "Think of the activation function like a simple decision tree:\n",
      "\n",
      "1. **Input**: The employee receives an order (input) from the previous node.\n",
      "2. **Processing**: The employee looks at the order and decides what to do with it based on their recipe book (activation function).\n",
      "3. **Output**: The employee produces a new order (output) based on their decision.\n",
      "\n",
      "There are several types of activation functions, each with its own \"recipe book.\" Here are a few examples:\n",
      "\n",
      "* **Sigmoid**: This is like a simple \"yes or no\" decision. If the input is above a certain threshold, the employee says \"yes\" (1), and if it's below, they say \"no\" (0).\n",
      "* **ReLU (Rectified Linear Unit)**: This is like a simple \"if-then\" statement. If the input is positive, the employee says \"yes\" (1), and if it's negative, they say \"no\" (0).\n",
      "* **Tanh (Hyperbolic Tangent)**: This is like a more complex \"if-then-else\" statement. If the input is above a certain threshold, the employee says \"yes\" (1), and if it's below, they say \"no\" (0), but with a twist.\n",
      "\n",
      "The activation function is crucial because it determines how the neural network learns and makes decisions. By adjusting the \"recipe book\" (activation function), the network can learn to recognize patterns and make predictions.\n",
      "\n",
      "In summary, the activation function is like a recipe book that tells each node in a neural network how to process information and produce an output. It's a critical component of the network's decision-making process, and different types of activation functions can be used to achieve different results."
     ]
    }
   ],
   "source": [
    "\n",
    "model_id = \"meta.llama3-1-8b-instruct-v1:0\"\n",
    "\n",
    "# Both of these models work. \n",
    "\n",
    "#meta.llama3-1-8b-instruct-v1:0\n",
    "#meta.llama3-70b-instruct-v1:0\n",
    "\n",
    "# Define the prompt for the model.\n",
    "prompt = \"Describe how the activation functions in a neural network work. Use language suitable for an executive with little technical knowledge.\"\n",
    "\n",
    "formatted_prompt = f\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "{prompt}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Format the request payload using the model's native structure.\n",
    "payload = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"max_gen_len\": 512,\n",
    "    \"temperature\": 0.0,\n",
    "}\n",
    "\n",
    "body = json.dumps(payload)\n",
    "\n",
    "response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "    modelId=model_id,\n",
    "    body=body\n",
    ")\n",
    "\n",
    "for event in response[\"body\"]:\n",
    "    chunk = json.loads(event[\"chunk\"][\"bytes\"])\n",
    "    if \"generation\" in chunk:\n",
    "        print(chunk[\"generation\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe3937d-3911-43b7-a22f-c7db09f5dcf2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "db8b58e2-3ad4-4906-bab2-beb7f3fc89aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, here are the general steps to apply for Canadian permanent residency as a Guatemalan citizen. Please note that the process can be complex and time-consuming, and it's always a good idea to consult with a Canadian immigration lawyer or consultant for personalized advice.\n",
      "\n",
      "1. **Determine your eligibility**: The first step is to determine if you're eligible to apply for Canadian permanent residency. There are several programs through which you can apply, including Express Entry, Provincial Nominee Programs, Quebec-selected skilled workers, and others. Each program has its own eligibility criteria.\n",
      "\n",
      "2. **Language Test**: You will need to prove your proficiency in either English or French. This usually involves taking a standardized test such as the IELTS for English or the TEF for French.\n",
      "\n",
      "3. **Education Credential Assessment (ECA)**: If you're applying through Express Entry, you'll likely need an ECA to show that your foreign education is equivalent to Canadian education standards.\n",
      "\n",
      "4. **Create an online profile**: Depending on the program you're applying under, you may need to create an online profile with the Canadian immigration authorities. For example, if you're applying through Express Entry, you'll need to create an Express Entry profile.\n",
      "\n",
      "5. **Receive an Invitation to Apply (ITA)**: If you're eligible and meet the criteria, you may receive an ITA. This is an invitation from the Canadian government to formally apply for permanent residency.\n",
      "\n",
      "6. **Submit your application**: Once you receive an ITA, you can submit your application for permanent residency. This will involve providing detailed personal information, as well as documents such as your passport, language test results, ECA, police certificates, and medical exam results.\n",
      "\n",
      "7. **Pay your fees**: You'll need to pay various fees, including processing fees and the Right of Permanent Residence Fee.\n",
      "\n",
      "8. **Wait for a decision**: Processing times can vary, but it generally takes several months for a decision to be made on a permanent residency application.\n",
      "\n",
      "9. **Prepare for arrival**: If your application is approved, you'll need to prepare for your arrival in Canada. This may involve things like finding a place to live, looking for a job, and making travel arrangements."
     ]
    }
   ],
   "source": [
    "\n",
    "# Set the model ID, e.g., Mistral Large.\n",
    "model_id = \"mistral.mistral-large-2402-v1:0\"\n",
    "\n",
    "# Define the prompt for the model.\n",
    "prompt = \"Give me the steps to apply for Canadien residency being a Guatemalan citizen.\"\n",
    "\n",
    "# Embed the prompt in Mistral's instruction format.\n",
    "formatted_prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
    "\n",
    "# Formatting the payload for this model\n",
    "\n",
    "payload = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"max_tokens\": 512,\n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "# Convert the native request to JSON.\n",
    "request = json.dumps(payload)\n",
    "\n",
    "streaming_response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "        modelId=model_id, body=request\n",
    "    )\n",
    "\n",
    "for event in streaming_response[\"body\"]:\n",
    "    chunk = json.loads(event[\"chunk\"][\"bytes\"])\n",
    "    if \"outputs\" in chunk:\n",
    "        print(chunk[\"outputs\"][0].get(\"text\"), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5216e18-dd7d-463e-8f44-2aef63a7033c",
   "metadata": {},
   "source": [
    "## Stability AI - Stable Diffusion is a model generally used for image generation and not text stream. Therefore, is not applicable for this section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
